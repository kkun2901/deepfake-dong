"""
MesoNet ë°±ì—”ë“œ
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms
from PIL import Image
import cv2
import numpy as np
from pathlib import Path
import threading
import time

from app.core.config import MESONET_WEIGHTS, IMAGE_SIZE, TORCH_NUM_THREADS, CV2_NUM_THREADS

# CPU ìŠ¤ë ˆë“œ ì„¤ì •
torch.set_num_threads(TORCH_NUM_THREADS)
cv2.setNumThreads(CV2_NUM_THREADS)

class Meso4(nn.Module):
    """MesoNet-4 ëª¨ë¸ êµ¬ì¡° (256x256 ìž…ë ¥ìš©, íŠœë‹ëœ ë²„ì „)"""
    def __init__(self, num_classes=2, dropout_rate=0.4):
        super(Meso4, self).__init__()
        
        # ì²« ë²ˆì§¸ ë¸”ë¡
        self.conv1 = nn.Conv2d(3, 8, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(8)
        self.conv2 = nn.Conv2d(8, 8, kernel_size=5, padding=2)
        self.bn2 = nn.BatchNorm2d(8)
        self.pool1 = nn.MaxPool2d(2, 2)  # 256 -> 128
        self.drop1 = nn.Dropout2d(dropout_rate)
        
        # ë‘ ë²ˆì§¸ ë¸”ë¡
        self.conv3 = nn.Conv2d(8, 16, kernel_size=5, padding=2)
        self.bn3 = nn.BatchNorm2d(16)
        self.conv4 = nn.Conv2d(16, 16, kernel_size=5, padding=2)
        self.bn4 = nn.BatchNorm2d(16)
        self.pool2 = nn.MaxPool2d(2, 2)  # 128 -> 64
        self.drop2 = nn.Dropout2d(dropout_rate)
        
        # ì„¸ ë²ˆì§¸ ë¸”ë¡
        self.conv5 = nn.Conv2d(16, 16, kernel_size=5, padding=2)
        self.bn5 = nn.BatchNorm2d(16)
        self.conv6 = nn.Conv2d(16, 16, kernel_size=5, padding=2)
        self.bn6 = nn.BatchNorm2d(16)
        self.pool3 = nn.MaxPool2d(2, 2)  # 64 -> 32
        self.drop3 = nn.Dropout2d(dropout_rate)
        
        # Fully Connected (256x256 -> 32x32 after 3 pools)
        self.fc1 = nn.Linear(16 * 32 * 32, 16)
        self.bn7 = nn.BatchNorm1d(16)
        self.drop4 = nn.Dropout(dropout_rate)
        self.fc2 = nn.Linear(16, num_classes)
    
    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = self.pool1(x)
        x = self.drop1(x)
        
        x = F.relu(self.bn3(self.conv3(x)))
        x = F.relu(self.bn4(self.conv4(x)))
        x = self.pool2(x)
        x = self.drop2(x)
        
        x = F.relu(self.bn5(self.conv5(x)))
        x = F.relu(self.bn6(self.conv6(x)))
        x = self.pool3(x)
        x = self.drop3(x)
        
        x = x.view(x.size(0), -1)
        x = F.relu(self.bn7(self.fc1(x)))
        x = self.drop4(x)
        x = self.fc2(x)
        
        return x

class MesoNetBackend:
    """MesoNet ë°±ì—”ë“œ"""
    
    def __init__(self):
        self.model = None
        self.device = torch.device("cpu")
        # íŠœë‹ëœ ëª¨ë¸ì— ë§žëŠ” ì „ì²˜ë¦¬ (í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ê²ƒê³¼ ë™ì¼)
        # Resize(256, 256) + ToTensor() + Normalize(-1~1)
        self.transform = transforms.Compose([
            transforms.Resize((256, 256)),  # íŠœë‹ëœ ëª¨ë¸ ìž…ë ¥ í¬ê¸°
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # -1~1 ì •ê·œí™”
        ])
        self.loading_lock = threading.Lock()
        self.model_loaded = False
    
    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        if self.model_loaded:
            return True
            
        with self.loading_lock:
            if self.model_loaded:
                return True
                
            try:
                # ëª¨ë¸ êµ¬ì¡° ìƒì„± (íŠœë‹ëœ ëª¨ë¸: 256x256 ìž…ë ¥, dropout=0.4)
                self.model = Meso4(num_classes=2, dropout_rate=0.4)
                
                # ëª¨ë¸ íŒŒì¼ í™•ì¸ ë° ë¡œë“œ
                if not Path(MESONET_WEIGHTS).exists():
                    print(f"âš  ê²½ê³ : MesoNet ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {MESONET_WEIGHTS}")
                    print(f"âš  ëžœë¤ ì´ˆê¸°í™”ëœ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.")
                    print(f"ðŸ’¡ ëª¨ë¸ íŒŒì¼ì„ ì„¤ì •í•˜ë ¤ë©´:")
                    print(f"   1. ê¸°ì¡´ ì»´í“¨í„°ì—ì„œ backend/weights/best_model_tuned.pt íŒŒì¼ì„ ë³µì‚¬")
                    print(f"   2. ë˜ëŠ” python backend/download_models.py ì‹¤í–‰")
                    self.model.to(self.device).eval()
                    self.model_loaded = True
                    return True
                
                # ê°€ì¤‘ì¹˜ ë¡œë“œ
                checkpoint = torch.load(MESONET_WEIGHTS, map_location=self.device)
                
                # ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡°ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ë¡œë“œ
                if isinstance(checkpoint, dict):
                    if 'model_state_dict' in checkpoint:
                        state_dict = checkpoint['model_state_dict']
                    elif 'state_dict' in checkpoint:
                        state_dict = checkpoint['state_dict']
                    elif 'model' in checkpoint:
                        state_dict = checkpoint['model']
                    else:
                        state_dict = checkpoint
                else:
                    state_dict = checkpoint
                
                # state_dict í‚¤ ì •ë¦¬
                cleaned_state_dict = {}
                for k, v in state_dict.items():
                    new_key = k.replace('module.', '').replace('model.', '')
                    cleaned_state_dict[new_key] = v
                
                self.model.load_state_dict(cleaned_state_dict, strict=True)
                self.model.to(self.device)
                self.model.eval()
                
                self.model_loaded = True
                return True
                
            except Exception as e:
                print(f"[MesoNet] ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                self.model = None
                self.model_loaded = False
                return False
    
    def preprocess_image(self, image_path: str, face_crop: bool = True):
        """
        ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ì–¼êµ´ crop + 256x256 ë¦¬ì‚¬ì´ì¦ˆ)
        
        Args:
            image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
            face_crop: ì–¼êµ´ crop ì‚¬ìš© ì—¬ë¶€
        
        Returns:
            tuple: (ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ í…ì„œ, ì–¼êµ´_ê°ì§€_ì—¬ë¶€)
        """
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            image = cv2.imread(image_path)
            if image is None:
                raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
            
            # BGR -> RGB ë³€í™˜
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # ì–¼êµ´ crop (ì˜µì…˜)
            face_detected = True
            if face_crop:
                image, face_detected = self._crop_face(image)
            
            # PIL Imageë¡œ ë³€í™˜
            pil_image = Image.fromarray(image)
            
            # ë¦¬ì‚¬ì´ì¦ˆ ë° ì •ê·œí™”
            image_tensor = self.transform(pil_image).unsqueeze(0)
            
            return image_tensor.to(self.device), face_detected
            
        except Exception as e:
            print(f"[MesoNet] ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            raise
    
    def _crop_face(self, image: np.ndarray):
        """ì–¼êµ´ ì˜ì—­ crop (í™”ë©´ ë…¹í™” ì˜ìƒì— ìµœì í™”)
        
        Returns:
            tuple: (cropped_image, face_detected)
                - cropped_image: cropëœ ì´ë¯¸ì§€
                - face_detected: ì–¼êµ´ ê°ì§€ ì—¬ë¶€ (bool)
        """
        try:
            # Haar Cascade ì–¼êµ´ ê°ì§€
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            face_cascade = cv2.CascadeClassifier(
                cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            )
            
            h, w = image.shape[:2]
            
            # ì—¬ëŸ¬ íŒŒë¼ë¯¸í„°ë¡œ ì–¼êµ´ ê°ì§€ ì‹œë„ (í™”ë©´ ë…¹í™” ì˜ìƒì˜ ê²½ìš° UI ì˜¤ë²„ë ˆì´ë¡œ ì¸í•´ ê°ì§€ê°€ ì–´ë ¤ìš¸ ìˆ˜ ìžˆìŒ)
            face_detection_params = [
                # íŒŒë¼ë¯¸í„° ì„¸íŠ¸ 1: ê¸°ë³¸ (ê°€ìž¥ ì—„ê²©)
                {
                    'scaleFactor': 1.1,
                    'minNeighbors': 5,
                    'minSize': (max(30, int(min(h, w) * 0.05)), max(30, int(min(h, w) * 0.05)))
                },
                # íŒŒë¼ë¯¸í„° ì„¸íŠ¸ 2: ë” ì™„í™” (ìž‘ì€ ì–¼êµ´ë„ ê°ì§€)
                {
                    'scaleFactor': 1.05,
                    'minNeighbors': 3,
                    'minSize': (max(20, int(min(h, w) * 0.03)), max(20, int(min(h, w) * 0.03)))
                },
                # íŒŒë¼ë¯¸í„° ì„¸íŠ¸ 3: ê°€ìž¥ ì™„í™” (UI ì˜¤ë²„ë ˆì´ê°€ ìžˆì–´ë„ ê°ì§€)
                {
                    'scaleFactor': 1.05,
                    'minNeighbors': 2,
                    'minSize': (max(15, int(min(h, w) * 0.02)), max(15, int(min(h, w) * 0.02)))
                }
            ]
            
            all_faces = []
            for params in face_detection_params:
                faces = face_cascade.detectMultiScale(
                    gray,
                    scaleFactor=params['scaleFactor'],
                    minNeighbors=params['minNeighbors'],
                    minSize=params['minSize'],
                    flags=cv2.CASCADE_SCALE_IMAGE
                )
                if len(faces) > 0:
                    all_faces.extend(faces)
            
            # ì¤‘ë³µ ì œê±° (ê²¹ì¹˜ëŠ” ì–¼êµ´ ì œê±°)
            if len(all_faces) > 0:
                # ê°€ìž¥ í° ì–¼êµ´ ì„ íƒ
                largest_face = max(all_faces, key=lambda x: x[2] * x[3])
                x, y, w_face, h_face = largest_face
                
                # ì–¼êµ´ í¬ê¸° ê²€ì¦: ì–¼êµ´ì´ ì´ë¯¸ì§€ì˜ ìµœì†Œ 1% ì´ìƒì„ ì°¨ì§€í•´ì•¼ í•¨ (2% -> 1%ë¡œ ì™„í™”)
                face_area = w_face * h_face
                image_area = h * w
                face_ratio = face_area / image_area
                
                if face_ratio < 0.01:  # ì–¼êµ´ì´ ë„ˆë¬´ ìž‘ìœ¼ë©´ ë¬´ì‹œ
                    # ì–¼êµ´ì´ ê°ì§€ë˜ì§€ ì•ŠìŒìœ¼ë¡œ ì²˜ë¦¬
                    size = min(h, w)
                    y = (h - size) // 2
                    x = (w - size) // 2
                    return image[y:y+size, x:x+size], False
                
                # ì–¼êµ´ ë¹„ìœ¨ ê²€ì¦: ì–¼êµ´ì˜ ê°€ë¡œ/ì„¸ë¡œ ë¹„ìœ¨ì´ 0.4~2.5 ë²”ìœ„ì—¬ì•¼ í•¨ (0.5~2.0 -> 0.4~2.5ë¡œ ì™„í™”)
                face_aspect_ratio = w_face / h_face if h_face > 0 else 0
                if face_aspect_ratio < 0.4 or face_aspect_ratio > 2.5:
                    # ë¹„ìœ¨ì´ ì´ìƒí•˜ë©´ ì–¼êµ´ì´ ì•„ë‹ ê°€ëŠ¥ì„± ë†’ìŒ
                    size = min(h, w)
                    y = (h - size) // 2
                    x = (w - size) // 2
                    return image[y:y+size, x:x+size], False
                
                # ì—¬ìœ  ê³µê°„ ì¶”ê°€ (20%)
                margin = int(min(w_face, h_face) * 0.2)
                x = max(0, x - margin)
                y = max(0, y - margin)
                w_face = min(image.shape[1] - x, w_face + 2 * margin)
                h_face = min(image.shape[0] - y, h_face + 2 * margin)
                
                # ì–¼êµ´ ì˜ì—­ crop
                face_image = image[y:y+h_face, x:x+w_face]
                return face_image, True
            else:
                # ì–¼êµ´ì´ ê°ì§€ë˜ì§€ ì•Šìœ¼ë©´ ì¤‘ì•™ crop
                size = min(h, w)
                y = (h - size) // 2
                x = (w - size) // 2
                return image[y:y+size, x:x+size], False
                
        except Exception as e:
            print(f"[MesoNet] ì–¼êµ´ crop ì˜¤ë¥˜: {e}, ì›ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©")
            return image, False
    
    def predict(self, image_path: str, face_crop: bool = True):
        """
        ì´ë¯¸ì§€ ì˜ˆì¸¡
        
        Args:
            image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
            face_crop: ì–¼êµ´ crop ì‚¬ìš© ì—¬ë¶€
        
        Returns:
            ì˜ˆì¸¡ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (face_detected í•„ë“œ í¬í•¨)
        """
        if not self.model_loaded:
            if not self.load_model():
                return {"error": "ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨"}
        
        try:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ì–¼êµ´ ê°ì§€ ì—¬ë¶€ í¬í•¨)
            image_tensor, face_detected = self.preprocess_image(image_path, face_crop=face_crop)
            
            # ì–¼êµ´ì´ ê°ì§€ë˜ì§€ ì•Šìœ¼ë©´ ì‹ ë¢°ë„ ë‚®ì€ REALë¡œ ì²˜ë¦¬
            if not face_detected:
                return {
                    "label": "REAL",
                    "score": 0.0,  # ì‹ ë¢°ë„ 0
                    "fake_prob": 0.0,  # FAKE í™•ë¥  0
                    "real_prob": 1.0,  # REAL í™•ë¥  1.0 (ì˜ë¯¸ ì—†ìŒ)
                    "model": "MesoNet",
                    "face_detected": False,
                    "warning": "ì–¼êµ´ì´ ê°ì§€ë˜ì§€ ì•Šì•„ ê³„ì‚°ì—ì„œ ì œì™¸ë©ë‹ˆë‹¤"
                }
            
            # ì¶”ë¡ 
            with torch.no_grad():
                outputs = self.model(image_tensor)
                probs = F.softmax(outputs, dim=1)
                fake_prob = probs[0][1].item()  # FAKE í´ëž˜ìŠ¤ í™•ë¥ 
                real_prob = probs[0][0].item()  # REAL í´ëž˜ìŠ¤ í™•ë¥ 
                confidence = max(fake_prob, real_prob)
                label = "FAKE" if fake_prob > 0.5 else "REAL"
            
            return {
                "label": label,
                "score": float(confidence),
                "fake_prob": float(fake_prob),
                "real_prob": float(real_prob),
                "model": "MesoNet",
                "face_detected": True
            }
            
        except Exception as e:
            print(f"[MesoNet] ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return {"error": str(e)}


